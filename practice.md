## Пошаговый план

| Шаг | Что сделать | Статус ✅/❌ |
| --- | --- | --- |
| **1. Fork & Setup**  | Клонировать шаблон репо, создать виртуальное окружение | ✅ |
| **2. Получить датасет** | Скопировать небольшой CSV с датасетом Титаник (ниже есть ссылки). Поместить в `data/raw/`.   | ✅ |
| **3. Pre-processing**  | Написать `make_dataset.py` <br>– чистка, <br>– разделение на train/val, <br>– сохранить в `data/processed/`. |   |
| **4. Exploratory Analysis**    | Создать jupyter-нотбук `notebooks/01_eda.ipynb` с быстрыми графиками распределений классов.  |   |
| **5. Реализовать Dataset & Dataloader** | `dataset.py` + проверочный тест в `tests/`.   |    |
| **6. Архитектура модели**  | Изучить шаблон в `ff_model.py` простая `nn.Sequential`. Реализовать считывание параметров модели (hidden\_units, drop\_rate) из файла `src/configs/config.yaml`.   |   |
| **7. Базовый цикл обучения**   | Изучить скелет модуля `train_loop.py`  и запустить 5-10 эпох на CPU.  |   |
| **8. Метрики**   | Добавить подсчет метрик accuracy, precision, recall, F1, ROC-AUC  |    |
<!-- | **9. Логирование**   | Подключить MLflow *или* W\&B. Логировать loss, accuracy/F1, конфиг.    |    |
| **10. Dropout + GradAccum**   | Добавить Dropout в архитектуру и аккумуляцию градиента (например, градиентный шаг каждые *k* батчей).    |   |
| **11. Hyper-parameter Search**  | В `utils/hpo.py`: простой loop по сетке learning\_rate × hidden\_units. Логировать лучшие результаты.  |  | -->
<!-- | **12. Итоговый отчёт**   | `reports/README.md`: <br>– кратко об архитектуре, <br>– скрины метрик, <br>– выводы (что улучшило результат).  |   |
| **13. Теория → README.md**   | Создать раздел «Theory Q\&A» и заполнить (см. список вопросов ниже).   | Проверка самостоятельного изучения.    |
| **14. Pull-Request**   | PR → code-review → merge в `main`.    | Завершение цикла разработки.   | -->

### Дополнительно:
- В начале лучше сфокусироваться на табличной задаче бинарной классификации. В качестве датасета можно использовать классический датасет Титаник. Чуть позже можно будет приступить к текстам и использовать датасет sms spam:
    - Тинаник: https://www.kaggle.com/datasets/yasserh/titanic-dataset
    - SMS Spam Collection Dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset
