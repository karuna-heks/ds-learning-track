

### Производная, градиент, частная производная

1. Что такое производная функции одной переменной?
    - Скорость изменения функции в конкретной данной точке
2. Что такое частная производная?
    - Скорость изменения функции при изменении одной из переменных,при этом остальные переменные не меняются
3. Что такое градиент?
    - Это вектор, который показывает направление, в котором функция увеличивается больше всего
5. Посчитай градиент функции f(x1, x2) = x1^2 + x1*x2
    - Частная производная по x1 = 2x1 + x2, частная производная по x2 = x1; соответственно градиент по функции f(x1, x2) = (2x1 + x2, x1).

### Градиентный спуск

1. Почему шаг делается «в сторону, противоположную градиенту»?
    - Потому что градиент показывает, в каком направлении функция растет быстрее всего, а при градиентном спуске нам нужно найти минимум функции.
2. Что влияент на скорость сходимости?
    - Шаг обучения, начальная точка, тип градиентного спуска.

### Batch vs Mini-batch

1. Что такое Батч в задачах ML?
    - Несколько примеров из обучающей выборки, которые обрабатываются за раз за один шаг при обучении модели.
2. На что влияет размер батча?
    - На скорость обучения, использование памяти (большие батчи требует большей памяти) и на качество обучения в целом (слишком большие батчи могут привести к переобучению модели).
3. Какие преимущества/недостатки Batch gradient descent, mini-batch gradient descent и stohactic gradient descent?
    - Batch gradient descent - на каждой итерации веса обновляются после того, как градиент считается по всему датасету. Преимущества - точность подсчета весов (т.к. подсчет идет по всему датасету), недостатки - тяжело и долго, нужно много памяти.
    - Stohastic gradient descent - на каждой итерации веса обновляются по одному случайному примеру. Премущества: работает быстрее, памяти требуется намного меньше. Недостатки: может неустойчиво сходиться.
    - Mini-batch gradient descent - баланс между BGD и SGD: быстрее, чем BGD, и менее шумный, чем SGD, но нужно подбирать оптимальный размер батча.

### Loss-function

1. Что такое Loss-function?
    - ответ
2. Какие Loss-function берут для бинарной классификации, многоклассовой классификации, регрессии?
    - ответ

### Backpropagation

1. Кратко опиши алгоритм обратного распространения ошибок.
    - ответ
2. Что хранит PyTorch в .grad после backward()?
    - ответ

### Activation functions

1. Назови 3 популярные функции активации и их плюсы/минусы.
    - ответ

### Optimizers

1. В чём принципиальная разница между SGD и Adam?
    - ответ

### Batch Normalization

1. Что нормализуется в BatchNorm?
    - ответ
2. Как это влияет на градиенты?
    - ответ

### Dropout

1. Почему Dropout уменьшает переобучение?
    - ответ
2. Что происходит на инференсе со слоем Dropout?
    - ответ

### Gradient Accumulation

1. В каких случаях это необходимо?
    - ответ

### Early Stopping

1. Для чего нужна ранняя остановка обучения?
    - ответ
2. Какие два критерия обычно используются для остановки?
    - ответ

### Regularization L1/L2

1. Что это?
    - ответ
