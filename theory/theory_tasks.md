

### Производная, градиент, частная производная

1. Что такое производная функции одной переменной?
    - Скорость изменения функции в конкретной данной точке
2. Что такое частная производная?
    - Скорость изменения функции при изменении одной из переменных,при этом остальные переменные не меняются
3. Что такое градиент?
    - Это вектор, который показывает направление, в котором функция увеличивается больше всего
5. Посчитай градиент функции f(x1, x2) = x1^2 + x1*x2
    - Частная производная по x1 = 2x1 + x2, частная производная по x2 = x1; соответственно градиент по функции f(x1, x2) = (2x1 + x2, x1).
    TO DO: Непрерывная функция - посмотреть, чем отличается от прерывной и какие есть проблемы с производными при подсчете:
- Прерывная функция в отличие от непрерывной имеет хотя бы одну точку разрыва, то есть если функция прерывна в этой точке, то для этой точки производная либо не существует, либо не определяется.

### Градиентный спуск

1. Почему шаг делается «в сторону, противоположную градиенту»?
    - Потому что градиент показывает, в каком направлении функция растет быстрее всего, а при градиентном спуске нам нужно найти минимум функции.
2. Что влияент на скорость сходимости?
    - Шаг обучения, начальная точка, тип градиентного спуска.

### Batch vs Mini-batch

1. Что такое Батч в задачах ML?
    - Несколько примеров из обучающей выборки, которые обрабатываются за раз за один шаг при обучении модели.
2. На что влияет размер батча?
    - На скорость обучения, использование памяти (большие батчи требует большей памяти) и на качество обучения в целом (слишком большие батчи могут привести к переобучению модели).
3. Какие преимущества/недостатки Batch gradient descent, mini-batch gradient descent и stohactic gradient descent?
    - Batch gradient descent - на каждой итерации веса обновляются после того, как градиент считается по всему датасету. Преимущества - точность подсчета весов (т.к. подсчет идет по всему датасету), недостатки - тяжело и долго, нужно много памяти.
    - Stohastic gradient descent - на каждой итерации веса обновляются по одному случайному примеру. Премущества: работает быстрее, памяти требуется намного меньше. Недостатки: может неустойчиво сходиться.
    - Mini-batch gradient descent - баланс между BGD и SGD: быстрее, чем BGD, и менее шумный, чем SGD, но нужно подбирать оптимальный размер батча.

### Loss-function

1. Что такое Loss-function?
    - Это функция, которая вычисляем разницу между предсказанным значением и истинным значением. При обучении мы стараемся ниминизировать функцию потерь, чтобы модель предсказывала значения как можно лучше.
2. Какие Loss-function берут для бинарной классификации, многоклассовой классификации, регрессии?
    - Для бинарной классификации берут бинарную кросс-энтропию (log loss), для многоклассовой - categorical crossentropy, а для регрессии - MSE, MAE.

### Backpropagation

1. Кратко опиши алгоритм обратного распространения ошибок.
    - Алгоритм обратного распространения ошибок применяется для корректировки весов сети; для этого в обратном направлении - от выходного слоя к первому считается градиент функции потерь. То есть, этот градиент показывает, насколько нужно изменить каждый параметр сети, чтобы уменьшить функцию потерь.
2. Что хранит PyTorch в .grad после backward()?
    - .backward() вычисляет производную функции потерь по отношению к данной переменной и сохраняет в .grad. 

### Activation functions

1. Назови 3 популярные функции активации и их плюсы/минусы.
    - ReLU - если входное значение <=0, то ReLU выдает ноль, в противном случае выдает входное значение. Плюсы в том, что она быстро считается и эффективна, но минусы в том, что могут появиться "мертвые" нейроны, те. если входное значение будет отрицательным, то relu выдаст 0 и они не будут обновляться.
    - Сигмоид - преобразует входные значения в диапазон от 0 до 1, часто используется для бинарной классификации, минусы в том, что подсчет более медленный и может произойти "затухание градиента", т.е. градиенты могут становиться очень маленькими при использовании этой функции активации.
    - Гиперболический тангенс - используется для разных задач (классификация, регрессия например), преобразует входные значение в диапазон от -1 до 1. Он более эффективен, чем сигмоидная функция, но тоже может произойти затухание градиента.

### Optimizers

1. В чём принципиальная разница между SGD и Adam?
    - ответ

### Batch Normalization

1. Что нормализуется в BatchNorm?
    - ответ
2. Как это влияет на градиенты?
    - ответ

### Dropout

1. Почему Dropout уменьшает переобучение?
    - ответ
2. Что происходит на инференсе со слоем Dropout?
    - ответ

### Gradient Accumulation

1. В каких случаях это необходимо?
    - ответ

### Early Stopping

1. Для чего нужна ранняя остановка обучения?
    - ответ
2. Какие два критерия обычно используются для остановки?
    - ответ

### Regularization L1/L2

1. Что это?
    - ответ
